{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Project origin\n",
    "sys.path.append(os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "import random\n",
    "import tiktoken\n",
    "import pandas as pd\n",
    "\n",
    "from enum import Enum\n",
    "from time import sleep\n",
    "from dotenv import load_dotenv\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "from langchain_core.prompts import  PromptTemplate\n",
    "from langchain_core.output_parsers import  StrOutputParser\n",
    "\n",
    "from prompts.general_prompts import FEW_SHOT_PROMPT, ZERO_SHOT_PROMPT\n",
    "\n",
    "load_dotenv()\n",
    "# os.environ[\"MISTRAL_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data and enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'intent': 'accept_reservations', 'text': 'let me know if grub burger takes reservations'}, {'intent': 'account_blocked', 'text': 'are there any problems with my bank account'}]\n",
      "[{'text': 'does village inn let you make reservations', 'intent': 'accept_reservations'}, {'text': 'can i make a reservation at chima steakhouse in chicago', 'intent': 'accept_reservations'}]\n"
     ]
    }
   ],
   "source": [
    "with open(\"../../dataset/llms/llm.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "with open(\"../../dataset/test.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "print(data[0:2])\n",
    "print(test_data[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt config and utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def few_shot(user_input):\n",
    "    intents = sorted(set(f\"- {item['intent']}\" for item in data))\n",
    "    intents_str = \"\\n\".join(intents)\n",
    "\n",
    "    examples_str = \"\\n\\n\".join(\n",
    "        [f'Phrase: \"{item[\"text\"]}\"\\nIntent: {item[\"intent\"]}' for item in data[:5]]\n",
    "    )\n",
    "\n",
    "    prompt_final = FEW_SHOT_PROMPT.format(\n",
    "        intents=intents_str, examples=examples_str, user_input=user_input\n",
    "    )\n",
    "    return prompt_final\n",
    "\n",
    "\n",
    "def zero_shot(user_input):\n",
    "    intents = sorted(set(f\"- {item['intent']}\" for item in data))\n",
    "    intents_str = \"\\n\".join(intents)\n",
    "\n",
    "    return ZERO_SHOT_PROMPT.format(intents=intents_str, user_input=user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(input):\n",
    "    encoder = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    num_tokens = len(encoder.encode(input))\n",
    "\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure LLMs and Evaluation Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MistralModels(str, Enum):\n",
    "    SMALL_LATEST = \"mistral-small-latest\"\n",
    "    MEDIUM_LATEST = \"mistral-medium-latest\"\n",
    "    LARGE_LATEST = \"mistral-large-latest\"\n",
    "\n",
    "class PrompType(str, Enum):\n",
    "    FEW_SHOT = \"few-shot\"\n",
    "    ZERO_SHOT = \"zero-shot\"\n",
    "    \n",
    "llm = ChatMistralAI(\n",
    "    model=MistralModels.SMALL_LATEST.value,\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\"{message}\")\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt_template | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_invoke(chain, message, retries=3, base_wait=2.5):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            return chain.invoke({\"message\": message})\n",
    "        except Exception as e:\n",
    "            if attempt < retries - 1:\n",
    "                wait_time = base_wait + random.uniform(0, 0.5)\n",
    "                print(f\"Error: {e}. Retrying in {wait_time:.2f}s...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                print(\"Failed after retries:\", e)\n",
    "                return \"ERROR\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_evaluation(\n",
    "#     test_data: dict,\n",
    "#     mode: PrompType = PrompType.FEW_SHOT,\n",
    "#     sleep_time: float = 2.5,\n",
    "#     verbose: bool = False,\n",
    "#     save_path: str = None  # checkpoint\n",
    "# ):\n",
    "#     results = []\n",
    "\n",
    "#     handler_mode: PrompType = {\n",
    "#         PrompType.FEW_SHOT: few_shot,\n",
    "#         PrompType.ZERO_SHOT: zero_shot,\n",
    "#     }\n",
    "\n",
    "#     if mode not in handler_mode:\n",
    "#         raise ValueError(\"Mode must be 'few-shot' or 'zero-shot'\")\n",
    "\n",
    "#     total_start = time.time()\n",
    "\n",
    "#     for i, item in enumerate(test_data):\n",
    "#         try:\n",
    "#             start_time = time.time()\n",
    "#             sleep(sleep_time)\n",
    "#             user_input = item[\"text\"]\n",
    "#             expected_intent = item[\"intent\"]\n",
    "\n",
    "#             prompt = handler_mode[mode](user_input)\n",
    "#             predicted_intent = safe_invoke(chain, prompt)\n",
    "#             duration = time.time() - start_time\n",
    "\n",
    "#             results.append(\n",
    "#                 {\n",
    "#                     \"text\": user_input,\n",
    "#                     \"expected\": expected_intent,\n",
    "#                     \"predicted\": predicted_intent.strip(),\n",
    "#                     \"latency_sec\": round(duration, 3)\n",
    "#                 }\n",
    "#             )\n",
    "\n",
    "#             if verbose:\n",
    "#                 print(f\"[{i+1}/{len(test_data)}] Latency: {round(duration, 2)}s\")\n",
    "\n",
    "#             # checkpoint partial\n",
    "#             if save_path:\n",
    "#                 df_partial = pd.DataFrame(results)\n",
    "#                 df_partial.to_csv(f\"{save_path}_partial.csv\", index=False)\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error to process índex {i}: {e}\")\n",
    "#             break \n",
    "\n",
    "#     total_duration = time.time() - total_start\n",
    "#     avg_latency = total_duration / len(results) if results else 0\n",
    "#     throughput = len(results) / total_duration if total_duration > 0 else 0\n",
    "\n",
    "#     stats = {\n",
    "#         \"total_time_sec\": round(total_duration, 3),\n",
    "#         \"avg_latency_sec\": round(avg_latency, 3),\n",
    "#         \"throughput_samples_per_sec\": round(throughput, 3),\n",
    "#     }\n",
    "\n",
    "#     if save_path:\n",
    "#         pd.DataFrame(results).to_csv(f\"{save_path}_final.csv\", index=False)\n",
    "#         with open(f\"{save_path}_stats.json\", \"w\") as f:\n",
    "#             json.dump(stats, f, indent=2)\n",
    "\n",
    "#     return results, stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "def load_partial_results(save_path):\n",
    "    partial_path = f\"{save_path}_partial.csv\"\n",
    "    if os.path.exists(partial_path):\n",
    "        df = pd.read_csv(partial_path)\n",
    "        texts_done = set(df[\"text\"].tolist())\n",
    "        return df.to_dict(orient=\"records\"), texts_done\n",
    "    return [], set()\n",
    "\n",
    "\n",
    "def run_evaluation_robust(\n",
    "    test_data: list,\n",
    "    mode: PrompType = PrompType.FEW_SHOT,\n",
    "    sleep_time: float = 2.5,\n",
    "    verbose: bool = False,\n",
    "    save_path: str = None\n",
    "):\n",
    "    results, already_processed = [], set()\n",
    "    errors = []\n",
    "\n",
    "    if save_path:\n",
    "        results, already_processed = load_partial_results(save_path)\n",
    "\n",
    "    handler_mode = {\n",
    "        PrompType.FEW_SHOT: few_shot,\n",
    "        PrompType.ZERO_SHOT: zero_shot,\n",
    "    }\n",
    "\n",
    "    if mode not in handler_mode:\n",
    "        raise ValueError(\"Mode must be 'few-shot' or 'zero-shot'\")\n",
    "\n",
    "    total_start = time.time()\n",
    "    total = len(test_data)\n",
    "\n",
    "    for i, item in enumerate(test_data):\n",
    "        user_input = item[\"text\"]\n",
    "\n",
    "        if user_input in already_processed:\n",
    "            if verbose:\n",
    "                print(f\"[{i+1}/{total}] ⏩ Skipped\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            sleep(sleep_time)\n",
    "            iter_start = time.time()\n",
    "            \n",
    "            prompt = handler_mode[mode](user_input)\n",
    "            predicted_intent = safe_invoke(chain, prompt)\n",
    "            duration = time.time() - iter_start\n",
    "\n",
    "            result = {\n",
    "                \"text\": user_input,\n",
    "                \"expected\": item[\"intent\"],\n",
    "                \"predicted\": predicted_intent.strip() if predicted_intent != \"ERROR\" else \"ERROR\",\n",
    "                \"latency_sec\": round(duration, 3)\n",
    "            }\n",
    "            results.append(result)\n",
    "\n",
    "            if verbose:\n",
    "                elapsed = time.time() - total_start\n",
    "                percent = 100 * (i + 1) / total\n",
    "                eta = timedelta(seconds=int((elapsed / (i + 1)) * (total - (i + 1))))\n",
    "                print(f\"[{i+1}/{total}] ✅ {percent:.2f}% - {round(duration, 2)}s - ETA: {eta}\")\n",
    "\n",
    "            if save_path:\n",
    "                pd.DataFrame(results).to_csv(f\"{save_path}_partial.csv\", index=False)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[{i+1}/{total}] ❌ Error: {e}\")\n",
    "            errors.append(item)\n",
    "\n",
    "            results.append({\n",
    "                \"text\": user_input,\n",
    "                \"expected\": item[\"intent\"],\n",
    "                \"predicted\": \"ERROR\",\n",
    "                \"latency_sec\": 0\n",
    "            })\n",
    "\n",
    "            if save_path:\n",
    "                pd.DataFrame(results).to_csv(f\"{save_path}_partial.csv\", index=False)\n",
    "                with open(f\"{save_path}_error.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump(errors, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    total_duration = time.time() - total_start\n",
    "    n_responses = len(results)\n",
    "\n",
    "    if not n_responses:\n",
    "        stats = {\n",
    "            \"total_time_sec\": 0,\n",
    "            \"avg_latency_sec_inference\": 0,\n",
    "            \"throughput_inference\": 0,\n",
    "            \"throughput_pipeline\": 0,\n",
    "        }\n",
    "    else:\n",
    "        sum_latencies = sum(r[\"latency_sec\"] for r in results)\n",
    "        avg_latency_inference = sum_latencies / n_responses\n",
    "        throughput_inference = n_responses / sum_latencies if sum_latencies else 0\n",
    "        throughput_pipeline = n_responses / total_duration if total_duration else 0\n",
    "\n",
    "        stats = {\n",
    "            \"total_time_sec\": round(total_duration, 3),\n",
    "            \"avg_latency_sec_inference\": round(avg_latency_inference, 3),\n",
    "            \"throughput_inference\": round(throughput_inference, 3),\n",
    "            \"throughput_pipeline\": round(throughput_pipeline, 3),\n",
    "        }\n",
    "\n",
    "    if save_path:\n",
    "        pd.DataFrame(results).to_csv(f\"{save_path}_final.csv\", index=False)\n",
    "        with open(f\"{save_path}_stats.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(stats, f, indent=2, ensure_ascii=False)\n",
    "        if errors:\n",
    "            with open(f\"{save_path}_error.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(errors, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    return results, stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'do you know what the latest is with my credit card application',\n",
       " 'intent': 'application_status'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens per few-shot 810\n",
      "Total Tokens few-shot 3645000\n",
      "Tokens per zero-shot 719\n",
      "Total Tokens zero-shot 3235500\n"
     ]
    }
   ],
   "source": [
    "user_input = test_data[100]\n",
    "\n",
    "few_shot_prompt = few_shot(user_input)\n",
    "zero_shot_prompt = zero_shot(user_input)\n",
    "\n",
    "test_data_len = len(test_data)  # 4500\n",
    "\n",
    "print(f\"Tokens per few-shot {count_tokens(few_shot_prompt)}\")\n",
    "print(f\"Total Tokens few-shot {count_tokens(few_shot_prompt) * test_data_len}\")\n",
    "print(f\"Tokens per zero-shot {count_tokens(zero_shot_prompt)}\")\n",
    "print(f\"Total Tokens zero-shot {count_tokens(zero_shot_prompt) * test_data_len}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate and Export LLM Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_BASE_PATH = \"../../results/llm\"\n",
    "\n",
    "# zero_results, zero_stats = run_evaluation(\n",
    "#     test_data=test_data[:10], # Change to full dataset\n",
    "#     mode=PrompType.ZERO_SHOT,\n",
    "#     sleep_time=2.5,\n",
    "#     verbose=False,\n",
    "#     save_path=f\"{LLM_BASE_PATH}/zero_shot\",\n",
    "# )\n",
    "\n",
    "# print(\"Zero-shot stats:\", zero_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Few-shot stats: {'total_time_sec': 15.002, 'avg_latency_sec_inference': 0.499, 'throughput_inference': 2.003, 'throughput_pipeline': 0.333}\n"
     ]
    }
   ],
   "source": [
    "few_results, few_stats = run_evaluation_robust(\n",
    "    test_data=test_data[:5], # Change to full dataset\n",
    "    mode=PrompType.FEW_SHOT,\n",
    "    sleep_time=2.5,\n",
    "    verbose=False,\n",
    "    save_path=f\"{LLM_BASE_PATH}/few_shot\",\n",
    ")\n",
    "\n",
    "print(\"Few-shot stats:\", few_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
